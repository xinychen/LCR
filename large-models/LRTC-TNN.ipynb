{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Rank Tensor Completion with Truncated Nuclear Norm (LRTC-TNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def mat2ten(mat, tensor_size, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(tensor_size.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, list(tensor_size[index]), order = 'F'), 0, mode)\n",
    "\n",
    "def svt_tnn(mat, alpha, rho, theta):\n",
    "    tau = alpha / rho\n",
    "    [m, n] = mat.shape\n",
    "    if 2 * m < n:\n",
    "        u, s, v = np.linalg.svd(mat @ mat.T, full_matrices = False)\n",
    "        s = np.sqrt(s)\n",
    "        idx = np.sum(s > tau)\n",
    "        mid = np.zeros(idx)\n",
    "        mid[:theta] = 1\n",
    "        mid[theta:idx] = (s[theta : idx] - tau) / s[theta : idx]\n",
    "        return (u[:,:idx] @ np.diag(mid)) @ (u[:, :idx].T @ mat)\n",
    "    elif m > 2 * n:\n",
    "        return svt_tnn(mat.T, alpha, rho, theta).T\n",
    "    u, s, v = np.linalg.svd(mat, full_matrices = 0)\n",
    "    idx = np.sum(s > tau)\n",
    "    vec = s[:idx].copy()\n",
    "    vec[theta : idx] = s[theta : idx] - tau\n",
    "    return u[:, :idx] @ np.diag(vec) @ v[:idx, :]\n",
    "\n",
    "def LRTC(dense_tensor, sparse_tensor, alpha, rho, theta, epsilon, maxiter):\n",
    "    \"\"\"Low-Rank Tenor Completion with Truncated Nuclear Norm, LRTC-TNN.\"\"\"\n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    pos_missing = np.where(sparse_tensor == 0)\n",
    "    pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "\n",
    "    X = np.zeros(np.insert(dim, 0, len(dim))) # \\boldsymbol{\\mathcal{X}}\n",
    "    T = np.zeros(np.insert(dim, 0, len(dim))) # \\boldsymbol{\\mathcal{T}}\n",
    "    Z = sparse_tensor.copy()\n",
    "    last_tensor = sparse_tensor.copy()\n",
    "    snorm = np.sqrt(np.sum(sparse_tensor ** 2))\n",
    "    it = 0\n",
    "    while True:\n",
    "        rho = min(rho * 1.05, 1e5)\n",
    "        for k in range(len(dim)):\n",
    "            X[k] = mat2ten(svt_tnn(ten2mat(Z - T[k] / rho, k), alpha[k], rho, int(np.ceil(theta * dim[k]))), dim, k)\n",
    "        Z[pos_missing] = np.mean(X + T / rho, axis = 0)[pos_missing]\n",
    "        T = T + rho * (X - np.broadcast_to(Z, np.insert(dim, 0, len(dim))))\n",
    "        tensor_hat = np.einsum('k, kmnt -> mnt', alpha, X)\n",
    "        tol = np.sqrt(np.sum((tensor_hat - last_tensor) ** 2)) / snorm\n",
    "        last_tensor = tensor_hat.copy()\n",
    "        it += 1\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Tolerance: {:.6}'.format(tol))\n",
    "        print('MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "        print('RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "        print()\n",
    "        if (tol < epsilon) or (it >= maxiter):\n",
    "            break\n",
    "\n",
    "    print('Total iteration: {}'.format(it))\n",
    "    print('Tolerance: {:.6}'.format(tol))\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print()\n",
    "\n",
    "    return tensor_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Time Series Imputation\n",
    "\n",
    "PeMS dataset is available at https://github.com/xinychen/transdim/tree/master/datasets/California-data-set.\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- On 30%/50%/70% missing data, $\\rho=10^{-4}$, and $\\theta=0.05$;\n",
    "- On 90% missing data, $\\rho=10^{-5}$, and $\\theta=0.05$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.7\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "dense_tensor = mat2ten(dense_mat, np.array([dim1, 288, 4 * 7]), 0)\n",
    "sparse_tensor = mat2ten(sparse_mat, np.array([dim1, 288, 4 * 7]), 0)\n",
    "del dense_mat, sparse_mat\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "alpha = np.ones(3) / 3\n",
    "rho = 1e-4\n",
    "theta = 0.05\n",
    "epsilon = 1e-4\n",
    "maxiter = 100\n",
    "tensor_hat = LRTC(dense_tensor, sparse_tensor, alpha, rho, theta, epsilon, maxiter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds.'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
